Name: PRATHIN REDDY JUNNUTHULA, 700#: 700741496

Group Members: AKHIL ABBURI, MANASA TELLA & PRATHIN REDDY JUNNUTHULA

Abstract: This paper provides a thorough theoretical examination of reinforcement learning (RL) algorithms, focusing on Q-Learning, Deep Q-Learning (DQN), Policy Gradient methods (such as REINFORCE), Actor-Critical methods (such as A2C and A3C), Proximal Policy Optimization (PPO), and Soft Actor-Critical (SAC) [1][2][4][14]. Despite the significant empirical successes of these algorithms, there is a significant theoretical knowledge deficit regarding their operation, failure mechanisms, and potential improvements. This research seeks to bridge this divide through a thorough theoretical examination of the mechanics of these algorithms. The purpose of this paper is to investigate and comprehend the critical properties and behaviors of these RL algorithms, such as convergence, stability, robustness, and scalability, among others. [3][5]. The initiative aims to cast light on why these algorithms work, when they fail, and how they can be optimized by deconstructing the fundamental principles underlying them. The results of this theoretical analysis are subsequently compared and contrasted, illuminating the unique strengths and shortcomings of each algorithm under consideration [7][8][11]. This comparative analysis will not only improve the understanding of each algorithm individually, but it will also provide insights into the algorithm that is best adapted for specific use cases, which would be advantageous for the machine learning and reinforcement learning community as a whole. Our investigation entails in-depth research into the mathematical foundations of the algorithms, examining aspects like convergence rates, approximation properties, and computational complexity [13][15]. Special emphasis is placed on the effect of hyperparameters on the performance of these algorithms in an effort to facilitate a more intuitive understanding of their practical implementation. Analytical derivations and empirical observations are utilized to provide a comprehensive comprehension of the inner workings of these algorithms. Moreover, we investigate the impact of environmental factors, such as external noise, non-stationarity, and dimensionality, on the operation of reinforcement learning (RL) algorithms - an aspect frequently neglected in traditional research but crucial to their practical implementation [6][12][17]. This aspect of the research helps illustrate the intricate relationship between environment, algorithm, and overall system performance, resulting in a deeper understanding of the behavior of RL algorithms. The examination permits the formulation of guidelines for the efficient application of RL algorithms on the basis of this profound theoretical comprehension. The ultimate goal of our research is to provide a comprehensive, theory-based guide for practitioners and researchers in the field that will make algorithm selection, refining, and implementation more informed and effective [6][9][18].

Please find the explanation video in the following link: https://ucmo0-my.sharepoint.com/:u:/g/personal/axa65481_ucmo_edu/EbiBDsVLSb5HkVTs07GSkZ0B780IF1FKv5numjgkz3QKHQ?e=gQ88a2
